<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Algoritmos e inteligencia artificial | Alejandro Pascual Hernández</title>

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;600&display=swap" rel="stylesheet">
  
  <meta 
    name="description" 
    content="Análisis detallado de expresión génica en cáncer de mama usando R y diferentes técnicas de reducción de la dimensionalidad. Proyecto de portfolio de Alejandro Pascual."
  >

  <link
    rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css"
  />

  <link rel="stylesheet" href="../../style-proyects.css" />
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-light.min.css">
  
</head>
<body>

  <header class="hero project-hero-short reveal">
    <div class="hero-content">
      <h1> Algoritmos e inteligencia artificial </h1>    
      <a href="../../../index.html" class="btn">Volver al inicio</a>
      <a href="../master.html" class="btn">Volver a las asignaturas</a>
    </div>
  </header>

  <main class="section">

    <div class="tech-stack-tags reveal">
      <span>R</span>
      <span>RStudio</span>
      <span>R Markdown</span>
      <span>MDS</span>
      <span>Isomap</span>
      <span>Umap</span>
      <span>Clustering</span>
    </div>

    <section class="reveal">
      <h2>Aplicación de técnicas de aprendizaje no supervisado sobre datos biológicos</h2>
      <p>
       El objetivo de esta actividad es determinar de forma justificada aquellos algoritmos 
        de aprendizaje no supervisado más eficientes a la hora de detectar patrones en el conjunto 
        de datos propuesto. En este caso el conjunto de unos datos sobre cáncer.
      </p>
    </section>

    <section class="key-findings reveal">
      <h3>MÉTODOS DE REDUCCIÓN DE LA DIMENSIONALIDAD</h3>
      <p>
        Estos son los métodos seleccionados y las conclusiones finales:
      </p>
      <ul>
        <li>
          <i class="fas fa-check-circle" aria-hidden="true"></i>
          <a href="#mds"><strong>MDS</strong></a> Multidimensional scaling
        </li>
        <li>
          <i class="fas fa-check-circle" aria-hidden="true"></i>
           <a href="#isomap"><strong>Isomap</strong></a> Isometric mapping. 
        </li>
        <li>
          <i class="fas fa-check-circle" aria-hidden="true"></i>
           <a href="#umap"><strong>Umap</strong></a> Uniform Manifold Approximation and Projection.
        </li>
        <li>
          <i class="fas fa-check-circle" aria-hidden="true"></i>
          <a href="#clustering"><strong>Clustering</strong></a> Método de clustering jerárquico aglomerativo.
        </li>
         <li>
          <i class="fas fa-check-circle" aria-hidden="true"></i>
          <a href="#conclusiones"><strong>Conclusiones</strong></a> Estas son las conclusiones finales de la actividad.
        </li>
      </ul>
    </section>

    <div class="r-markdown-container reveal">
      
      <div id="pasos-previos" class="section level3">
        <h3>Pasos previos</h3>
        <p>En primer lugar, tenemos que cargar todas la librerías y datos que vamos a usar para el análisis de 
          datos. Por un lado, cargamos los datos de "data.csv" como una matriz en el que la fila 1 corresponde 
          a las muestras de nuestros pacientes y debemos indicar que en nuestra matriz no será una columna de genes 
          más sino los nombres. "labels.csv" corresponde a un data frame que asocia los nombres de estas muestras con el 
          tipo de cáncer que tiene el paciente al que corresponde esta muestra. </p>
        
        <pre><code class="r">library(plotly)
library(stats)
library(uwot)
library(RDRToolbox)
library(dplyr)
library(factoextra)
library(ggdendro)
library(cluster)
library(ggplot2) 
library(gridExtra)
library(pheatmap)
library(tidyr)

labels.raw &lt;- read.csv('labels.csv')
data &lt;- as.matrix(read.csv("data.csv", row.names = 1))</code></pre>

        <p>Como podemos ver en la matriz creada tenemos 801 muestras de pacientes y 20532 genes estudiados, lo cual nos lleva a 
          que haya un claro problema de dimensionalidad. Para solucionarlo debemos recurrir a distintas técnicas o herramientas que 
          nos permitan reducir todos estos datos a un nivel de dimensiones comprensibles e interpretables. Existen muchas de estas 
          técnicas que podemos utilizar, pero en esta actividad nos enfocaremos en 4 distintas que hemos escogido para enseñar diferentes 
          problemas que nos podemos encontrar a la hora de reducir la dimensionalidad. Iremos argumentando el porqué de nuestra elección 
          y cómo ha sido el tratamiento tanto de los datos como de los parámetros que se pueden modificar en cada método.</p>
      </div>

      <div id="mds" class="section level3">
        <h3>MDS</h3>
        <p>El primero de los métodos que decidimos utilizar es el MDS (multidimensional scaling, en español escalamiento multidimensional). 
          Se trata de un método en el cual se mide la distancia entre distintas muestras o puntos y los agrupa. Esta distancia puede ser medida
          de diferentes formas. Aunque nosotros en este caso decidimos usar la más común, la distancia euclidiana, también existen distancias como 
          la “de Manhattan”, “de Hamming”, etc. Este método nos permite seleccionar también el número de dimensiones en el que vamos a representar 
          nuestros resultados. En un principio los visualizaremos en 2 dimensiones. </p>
        
        <pre><code class="r">distances &lt;- dist(data, method = 'euclidean') # matriz de distancias

mds.results &lt;- cmdscale(d=distances, eig=TRUE, k=2, x.ret=TRUE) # La función cmdscale realiza el MSD

varianza.explicada &lt;- mds.results$eig/sum(mds.results$eig) * 100 # varianza explicada

mds.df &lt;- data.frame(mds.results$points) # convertimos los datos del mds en un data frame

ggplot(mds.df, aes(x=X1, y=X2, color=labels.raw$Class)) + #Por último graficamos
  geom_point(size=3) + 
  scale_color_manual(values=c("red", "blue", "green", "orange", "purple")) +
  labs(title="MDS - Types of Cancer", x="Dimension 1 (X1)", y="Dimension 2 (X2)", color = "Grupo") +
  theme_classic() + 
  theme(panel.grid.major = element_line(color = "gray90"), panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"), plot.title=element_text(hjust=0.5))
</code></pre>
        
        <figure class="r-plot-figure">
          <img src="fig1.png" alt="Fig. 1: MDS 2 dimensiones">
          <figcaption><em>Fig. 1: Representación gráfica de nuestros datos usando la técnica MDS en 2 dimensiones.</em></figcaption>
        </figure>

        <p>Como podemos ver en la representación gráfica existen muchos puntos que no están bien separados y aunque ya se pueden apreciar 
          nuestros 5 clusters de las 5 enfermedades diferentes, no creemos que esto sea una buena separación. Por este motivo decidimos intentar 
          visualizarlo en 3 dimensiones para comprobar si de esta manera podríamos diferenciar mejor los diferentes grupos. Para ello debemos
          cambiar el parámetro k, que equivale al número de dimensiones.</p>
         
         <pre><code class="r">mds_result &lt;- cmdscale(distances, k = 3) # hacemos lo mismo pero para 3 dimensiones

mds_df &lt;- as.data.frame(mds_result)
colnames(mds_df) &lt;- c("Dim1", "Dim2", "Dim3")

mds_df$Cancer_Type &lt;- labels.raw$Class # añadimos las etiquetas del cancer

fig &lt;- plot_ly(mds_df, x = ~Dim1, y = ~Dim2, z = ~Dim3, #graficamos
                color = ~Cancer_Type, colors = "Set1", 
                marker = list(size = 4))

fig </code></pre>
        <figure class="r-plot-figure">
          <img src="fig2.png" alt="Fig. 2: MDS 3 dimensiones">
          <figcaption><em>Fig. 2: Representación gráfica de nuestros datos usando la técnica MDS en 3 dimensiones.</em></figcaption>
        </figure>
        <p>Visualizando los resultados en 3 dimensiones podemos ver más claramente los 5 grupos diferenciados. Esto es una manera muy 
          interesante de resolver el problema inicial, pero conlleva otra serie de desventajas. La primera y más evidente es que no 
          siempre podremos representar con gráficas 3D pues no todos los medios los permiten y mucho menos si no son digitales. Otras 
          de las desventajas del MDS es que puede ser muy costoso computacionalmente ya que requiere calcular la distancia de cada uno 
          de los puntos entre ellos. </p>
      </div>
      
      <div id="isomap" class="section level3">
        <h3>Isomap</h3>
        <p>Como siguiente método vamos a utilizar uno relacionado con el anterior, el Isometric mapping (Isomap). Al igual que en el método 
          anterior se busca medir distancias entre puntos para realizar agrupaciones, pero en esta ocasión la distancia utilizada es la distancia 
          geodésica. Esta distancia se calcula sobre una superficie que cambia según sea la disposición del conjunto de datos. Esta disposición
          del conjunto de datos está determinada principalmente por lo que nosotros consideremos el número de vecinos próximos o K. 
          Este parámetro es el principal con el que jugaremos para obtener un mejor resultado a la hora de realizar agrupaciones. 
          En un principio el número de K que usaremos será 5.</p>
        
        <pre><code class="r">set.seed(1234)

isomap.results = Isomap(data=data, dims=1:10, k=5, plotResiduals=FALSE) # isomap de 1 a 10 dimensiones y con 5 vecinos. 

isomap.df &lt;- data.frame(isomap.results$dim2) # Dataframe con los puntos que queremos dibujar en el plano 2D

ggplot(isomap.df, aes(x = X1, y = X2, color = labels.raw$Class)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("red", "blue", "green", "orange", "purple")) +
  labs(title = "Isomap - Types of Cancer", x = 'Dim 1', y = 'Dim 2', color = "Grupo") +
  theme_classic() +
  theme(panel.grid.major = element_line(color = "gray90"), panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"), plot.title=element_text(hjust=0.5)) </code></pre>
        <figure class="r-plot-figure">
          <img src="fig3.png" alt="Fig. 3: Isomap 5 vecinos">
          <figcaption><em>Fig. 3: Representación gráfica de nuestros datos usando la técnica de Isomap con un parámetro de k (nº de puntos vecinos) de 5.</em></figcaption>
        </figure>
        <p>Con k=5 vemos que la separación no es muy buena por lo que decidimos ir aumentando este número hasta conseguir un mejor resultado.
          Aumentando llegamos a K=15 en el cual nos aparece el siguiente resultado.</p>
        
        <pre><code class="r">set.seed(1234)

isomap.results = Isomap(data=data, dims=1:10, k=15, plotResiduals=FALSE) # esta vez k=15

isomap.df &lt;- data.frame(isomap.results$dim2) 

ggplot(isomap.df, aes(x = X1, y = X2, color = labels.raw$Class)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("red", "blue", "green", "orange", "purple")) +
  labs(title = "Isomap - Types of Cancer", x = 'Dim 1', y = 'Dim 2', color = "Grupo") +
  theme_classic() +
  theme(panel.grid.major = element_line(color = "gray90"), panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"), plot.title=element_text(hjust=0.5))
</code></pre>
        
        <figure class="r-plot-figure">
          <img src="fig4.png" alt="Fig. 4: Isomap 15 vecinos">
          <figcaption><em>Fig. 4: Representación gráfica de nuestros datos usando la técnica de Isomap 
            con un parámetro de k (nº de puntos vecinos) de 15.</em></figcaption>
        </figure>

        <p>Aquí ya podemos observar una clara separación por grupos de nuestras 5 enfermedades. Este método pues resulta 
          efectivo para nuestro propósito, sin embargo, encontrar el parámetro para K adecuado puede ser 
          muy difícil y es posible que en algunos casos no 
          se llegue a encontrar. Por lo que este método puede tener esa limitación.</p>
      </div>
      
      <div id="umap" class="section level3">
        <h3>Umap</h3>
        <p>Ahora decidimos utilizar el método de Uniform Manifold Approximation and Projection (UMAP). Este es un método de 
          reducción de la dimensionalidad muy complejo que asume que la muestra de datos disponibles está distribuida de manera 
          uniforme por nuestro espacio topológico. Este espacio puede aproximarse y proyectarse sobre un espacio de menor dimensión.
          Para esto primero busca definir la estructura del espacio topológico y después busca cómo representar esta estructura de
          manera más adecuada en un espacio de menos dimensiones.</p>
        
        <pre><code class="r">set.seed(1234)

umap.results &lt;- umap(data, n_neighbors=0.15 * nrow(data), #Aquí hemos decidido no tocar los parámetros a excepción del nº de vecinos cercanos
                                                      # que es el parámetro n_neighbours
                     n_components = 2, min_dist = 0.1, local_connectivity=1, 
                     ret_model = TRUE, verbose = TRUE)

umap.df &lt;- data.frame(umap.results$embedding)

m_dist &lt;- dist(umap.df)

ggplot(umap.df, aes(x = X1, y = X2, color = labels.raw$Class)) +
  geom_point(size = 3) +
  scale_color_manual(values = c("red", "blue", "green", "orange", "purple")) +
  labs(title = "Método UMAP Types of Cancer", x = "X1", y = "X2", color = "Grupo") +
  theme_classic() +
  theme(panel.grid.major = element_line(color = "gray90"), panel.grid.minor = element_blank(),
        panel.background = element_rect(fill = "gray95"), plot.title=element_text(hjust=0.5))
        </code></pre>
        
        <figure class="r-plot-figure">
          <img src="fig5.png" alt="Fig. 5: Umap">
          <figcaption><em>Fig. 5: Representación gráfica de nuestros datos usando la técnica de Umap sin alterar ningún parámetro excepto el número 
            de vecinos cercanos (n_neighbors) que lo situamos en 0.15.</em></figcaption>
        </figure>

        <p>El método U-map es muy versátil y eficaz, pero a su vez esto hace que la complejidad de su manejo aumente. Como se puede observar en el código
          existen un gran número de parámetros que se pueden modificar con el fin de conseguir un resultado optimo. Por esta razón se necesita una comprensión 
          muy profunda del funcionamiento del método para poder optimizarlo al máximo o si no esta complejidad y versatilidad puede volverse en nuestra contra. 
          Aunque como se puede observar en la representación gráfica la separación en grupos es muy buena y eficiente. Mucho más que la de los métodos anteriores. </p>
      </div>

      <div id="clustering" class="section level3">
        <h3>Clustering</h3>
        <p>Por último, para completar esta actividad decidimos utilizar un método de clustering, más concretamente el método jerárquico aglomerativo. Elegimos 
          representar mediante un dendrograma nuestros resultados ya que es una forma diferente con respecto a lo que hemos visto hasta ahora en esta actividad. 
          Un dendrograma es una representación gráfica en forma de árbol invertido que muestra la manera en la que un conjunto de datos se agrupa en función de su 
          similitud. Cada “hoja” del árbol representa un elemento individual, en nuestro caso pacientes, y los nodos intermedios representan los grupos o clusters 
          que se forman. 
          Para interpretar un dendrograma es importante también observar la altura a la que las ramas de dos hojas se fusionan por primera vez. Cuanto menor sea esta altura 
          significa que más relacionadas están. Por lo tanto, la medida de relación entre dos hojas se mide en el eje vertical, no el horizontal. 
          Para la clusterización aglomerativa el dendrograma se construye con un enfoque bottom-up, es decir, cada una de nuestras observaciones o muestras constituye un 
          grupo propio el cual se va uniendo a otros grupos a medida que avanza la jerarquía. Estas uniones se van haciendo dependiendo de la distancia que se calcule 
          entre los grupos. Esta distancia puede ser medida de 4 formas diferentes y es un parámetro que podemos cambiar en nuestro código que nos dará como resultado 
          4 dendrogramas diferentes.</p>
        <p>Esto lo tendremos en cuenta a la hora de realizar nuestro dendrograma.</p>
        
        <pre><code class="r">df &lt;- read.csv("data.csv") # esta vez leemos el .csv como data frame

df_genes &lt;- df %&gt;% dplyr::select(starts_with("gene_")) # eliminamos la columna de los nombres de las muestras dejando solo la de los genes

df_genes_scale &lt;- scale(df_genes)  # Normalizamos
        </code></pre>
        <p>A la hora de realizar el clustering <strong> nos saltaron varios fallos en este punto</strong>. La causa de estos fallos es que el data frame de datos normalizados 
          presentaba varios genes (columnas enteras) con NaN, es decir, sin datos. Por eso primero debemos eliminar todos estos NaN antes de realizar el clustering.</p>
        
        <pre><code class="r">df_genes_scale_tibble &lt;- as_tibble(df_genes_scale)
df_clean_final &lt;- df_genes_scale_tibble %&gt;% select(where(~ !all(is.na(.)))) %&gt;% drop_na() # eliminamos todas las posibles NAs

dist_matrix &lt;- dist(df_clean_final) # matriz de distancia

# Se ejecuta el algoritmo de clusterización jerárquica aglomerativa usando los 4 métodos de linkage distintos

hclust_model_single &lt;- hclust(dist_matrix, method = "single") 

hclust_model_complete &lt;- hclust(dist_matrix, method = "complete") 

hclust_model_average &lt;- hclust(dist_matrix, method = "average") 

hclust_model_ward &lt;- hclust(dist_matrix, method = "ward.D") 

colors &lt;- rainbow(5) # colores en los dendrogramas

clust_single &lt;- fviz_dend(hclust_model_single, 
                          cex = 0.5,
                          k = 5,
                          palette = colors,
                          main = "Single",
                          xlab = "Índice de Observaciones",
                          ylab = "Distancia") + theme_classic()

clust_complete &lt;- fviz_dend(hclust_model_complete, 
                            cex = 0.5,
                            k = 5,
                            palette = colors,
                            main = "Complete",
                            xlab = "Índice de Observaciones",
                            ylab = "Distancia") + 
  theme_classic()

clust_average &lt;- fviz_dend(hclust_model_average, 
                           cex = 0.5,
                           k = 5,
                           palette = colors,
                           main = "Average",
                           xlab = "Índice de Observaciones",
                           ylab = "Distancia") + 
  theme_classic()

clust_ward &lt;- fviz_dend(hclust_model_ward, 
                        cex = 0.5,
                        k = 5,
                        palette = colors,
                        main = "Ward",
                        xlab = "Índice de Observaciones",
                        ylab = "Distancia") + 
  theme_classic()

grid.arrange(clust_single, clust_complete, clust_average, clust_ward, nrow = 2) </code></pre>
        
        <figure class="r-plot-figure">
          <img src="fig6.png" alt="Fig. 6: clustering 4">
          <figcaption><em>Fig. 6: Representación de los cuatro tipos de dendrogramas que se pueden 
            obtener usando el método de clustering jerárquico aglomerativo, cambiando el parámetro del método de linkage usado</em></figcaption>
        </figure>

        <p>Como hemos mencionado antes se crean 4 dendrogramas distintos dependiendo del método de linkage usado. Para los objetivos de esta actividad
          buscamos clusters compactos y homogéneos, que tengan muy poca variabilidad interna. Por este motivo el método de Ward es el que nos muestra mejores
          resultados ya que es un método que busca la menor varianza interna dentro de un cluster.  
          El dendrograma resultante es el siguiente, en él se pueden observar 5 clusters bien diferenciados y compactos que nos ayudan a la hora de interpretar nuestro 
          conjunto de datos iniciales. </p>
        <figure class="r-plot-figure">
          <img src="fig7.png" alt="Fig. 7: clustering ward">
          <figcaption><em>Fig. 7: Dendrograma de nuestros datos usando la técnica de clustering jerárquico aglomerativo usando a su vez el método de linkage
            de Ward.</em></figcaption>
        </figure>

        <div id="conclusiones" class="section level3">
          <h3>Conclusiones</h3>
          <p>Partimos con ventaja al realizar estos análisis ya que conocemos nuestro conjunto de datos. Se trata de lecturas de expresión de 20532 genes 
            realizados sobre 801 pacientes. Estos pacientes han sido diagnosticados con un tipo de cáncer, 5 tipos diferentes en total. Por lo que nuestra 
            búsqueda está enfocada en encontrar cuales de estos métodos nos permiten agrupar y proyectar en gráficas los grupos o clusters que generan todos 
            los pacientes diagnosticados con un mismo tipo de cáncer. Por esta razón decimos que partimos con ventaja ya que <strong>nos enfocaremos solo en aquellos
            resultados en los que se vea una clara separación de 5 grupos diferentes y equivalentes a los 5 tipos de cáncer</strong>. Solo este tipo de resultados 
            tendría un sentido biológico para nosotros y podrían sernos útiles. Una vez seleccionados los cuatro tipos de métodos y habiéndolos probado sobre 
            nuestros datos podemos resaltar algunos aspectos interesantes de cada uno de ellos:</p>
          <p>1. MDS: podemos observar como este método es fácilmente interpretable y <strong>bastante versátil</strong> ya que podemos jugar con varios parámetros, como el
            <strong>número de dimensiones</strong> (Fig 1 vs. 2) o el <strong>tipo de distancia calculada</strong>, dependiendo del tipo de datos con el que estemos tratando. 
            Pero a su vez podemos observar que para nuestro conjunto de datos <strong>no ha sido el más efectivo</strong>.</p>
          <p>2. Isomap: Este método nos dio mejores resultados que el anterior y también lo consideramos de la misma versatilidad. El punto que nosotros decidimos destacar es
            que <strong>puede llegar a ser muy complicado ajustar el parámetro de k para obtener unos buenos resultados</strong> (Fig. 3 y 4).</p>
          <p>3. Umap: Este es <strong>el método que mejores resultados proporciona</strong> (Fig. 5) y, con diferencia, <strong>el más versátil</strong>. Pero esta versatilidad es a 
            costa de tener muchos parámetros que definir y por lo tanto <strong>se necesita un conocimiento muy profundo del método para poder utilizarlo con la mayor 
            precisión</strong>. Es por esto que nos parece también <strong>el método menos accesible inicialmente</strong>.</p>
          <p>4. Clustering jerárquico aglomerativo: este método nos permitió <strong>observar el conjunto de datos desde otra perspectiva</strong> ya que los dendrogramas muestran de una manera 
            más visual cual es el <strong>grado de relación entre distintos clusters</strong>. En esta ocasión destacamos que <strong>es muy importante conocer nuestro conjunto de datos y los objetivos 
            que buscamos para elegir bien el método de linkage a la hora de realizar el clustering</strong> (Fig. 6 y 7).</p>
          <p>Por último, también añadiremos que todos estos métodos han sido <strong>muy demandantes computacionalmente</strong>. Una posible solución a esto podría ser seleccionar 
            un menor número de genes, por ejemplo, los 500 primeros, o seleccionar métodos menos demandantes. </p>
        </div>
      </div>
      </div>
     
    <section>
      <h3 class="section-subtitle">Enlaces</h3>
      <div class="links-container">
        
        <a
          href="https://github.com/Alex-PH-Lau/Alex-PH-Lau.github.io/blob/main/proyectos/master/ai/ai.html"
          target="_blank"
          rel="noopener noreferrer"
          class="link-card"
        >
          <i class="fab fa-github" aria-hidden="true"></i>
          <span>Ver en GitHub</span>
        </a>
      </div>
    </section>
  </main>

  <footer>
    <p>© 2025 Alejandro Pascual Hernández. Hecho en HTML/CSS/JS.</p>
  </footer>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/r.min.js"></script>
  
  <script>
    document.addEventListener('DOMContentLoaded', (event) => {
      document.querySelectorAll('pre code').forEach((el) => {
        hljs.highlightElement(el);
      });
    });
  </script>
  
  <script src="../../../script.js"></script>
</body>
</html>
